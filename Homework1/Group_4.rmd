---
title: "Group 4 - Salary Prediction"
author: "Akash Gobalarajah, Cyril Alain Scheurmann, Keijo Alexander Nierula, Roman Krass"
date: "2024.03.14"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE, echo=FALSE}
# Getting started by changing the default output of echo to TRUE for the current document
knitr::opts_chunk$set(echo = TRUE)

# Create a list of packages to install and load into the work space
libraries <- c("readr", "tidyverse", "dlookr", "ggplot2", "xgboost", "dplyr", "caret", "doParallel", "mltools", "knitr", "DT")

# Install packages from the predefined libraries list
lapply(libraries, function(x) {
  if (!(x %in% installed.packages())) {
    install.packages(x)
  }
})

# Load libraries
lapply(libraries, library, quietly = TRUE, character.only = TRUE)

# Remove current environment
rm(list = ls())

# scientific notation: off
options(scipen = 999)

# Set working directory
# setwd("C:/Users/roman/documents/repos/SBD3/Homework1")
# setwd("C:/Users/roman/documents/github/SBD3/Homework1")
```

# Project Setup {.unnumbered}

```{r importData}
# Set seed
set.seed(7)

# Importing data
load("data_wage.RData")
salaryData <- data
```

# Checking data for NA values {.unnumbered}
In this part we check our data for missing values. This is important because missing values can cause problems in the analysis and can lead to wrong results. 

<details>
<summary>Click to expand NA results</summary>


```{r naValues, out.width = "100%"}
# Check for missing values in each column of the data set and store the result in the "has_NAs" data frame
has_NAs <- as.data.frame(apply(data, 2, function(x) any(is.na(x))))

# Map matching column names for a more readable output
colnames(has_NAs) <- c("has_NA")

# Print the has_NAs data frame to check if there are any NA values in our data set
has_NAs
```
</details>
As we can see from the output above none of the columns contain any missing values. This is very good so we don't have to deal with the problem of missing values.


# Data description

``` {r}
dim(data)
```
The `dim` function shows us, that we have 10'809 and 78 variables in our data set. We have so many variables because the data we have has already one hot encoded some variables.

<details>
<summary>Click to expand to see data structure</summary>

```{r scructure}
# Check the structure of the data
str(data)
```
</details>
  
<details>
<summary>Click to expand to see the summary of all variables</summary>

```{r summary()}
# Check the summary of the data
summary(data)
```
</details>
When we look at the summary the variable `wage` has a minimum value of 0 and a maximum value of 551'774. This is a very big range and we have to check if there are any outliers in the data set. The big range can also be caused, because the dataset includes data from students to experts which indeed can have a big range in their salary.
All other variables look good. This is because a lot of variables are categorical variables which were one hot encoded. 

## How many numeric and how many categorical variables are included in the data? What categorical variable has the most levels in it?

```{r}
num_vars <- sum(sapply(data, is.numeric))
cat_vars <- sum(sapply(data, is.factor))

# Print counts
cat("Number of numeric variables:", num_vars, "\n")
cat("Number of categorical variables:", cat_vars, "\n")
```

Reporting the structure of the data set shows us, that we have `r as.character(cat_vars)` character columns and `r as.character(num_vars)` number columns.
Because we have `r as.character(cat_vars)` we cannot use all models because they only work with numeric data. If we could like to use a model like XGBoost we have to encode these categorical variables with one hot encoding so we end with only numeric variables.

Eventuell noch `diagnose` benutzen

## Check dependent variable wage

```{r}
# Plot histogram of wage
ggplot(data, aes(x = wage)) +
  geom_histogram(bins = 60, fill = "lightblue", color = "black") +
  scale_x_continuous(breaks = seq(min(data$wage), max(data$wage), by = 50000)) +
  labs(title = "Histogram of wage", x = "Wage", y = "Frequency") +
  theme_minimal(base_size = 17)
```

When we look at the histogram of the wage we can see that that most of the values are between 0 and 150'000. There are also some values between 150'000 and 250'000. The values above 250'000 are very rare. This is also a bit as expected because wages above 200'000 are not so common as wages to 150'000.

## Visualization to display the distribution of the categorial features

<details>
<summary>Click to expand variable distribution</summary>

```{r distribution-categorial}
# Function to plot bar plots for each categorical variable
plot_categorical_associations <- function(data) {
  categorical_vars <- sapply(data, is.factor) # or is.factor, depending on how data is loaded
  data_categorical <- data[, categorical_vars]

  for (var in names(data_categorical)) {
    # Create the plot and assign it to a variable
    plot <- ggplot(data, aes(x = !!sym(var))) +
      geom_bar(position = "stack") +
      labs(title = paste("Distribution of", var), x = var, y = "Count") +
      theme_minimal(base_size = 17) +
      scale_fill_manual(values = c("#105F77", "#f4a460")) +
      theme(
        axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 18)
      )

    # Print the plot
    print(plot)
  }
}

# Apply the function to the loan dataset
plot_categorical_associations(data)
```

</details>

# Model creation

## Splitting the data into training and test set
The first step before we can start with the model creation is to split the data into a training and a test set. This is important because we need to test our model on data it has never seen before. This is important to see if the model is able to generalize to new data. It also helps to identify overfitting if the model is overfitted to the training data.
```{r}
# First we need to split the data into a training and a test set
set.seed(7)

# Split the data into 70% training and 30% test data
trainIndex <- createDataPartition(data$wage, p = .7, list = FALSE)
data_train <- data[trainIndex, ]
data_test <- data[-trainIndex, ]
```

## Model creation with neural network

```{r}
# Scale test and train data
data_scaled_train <- as.data.frame(sapply(data_train[, sapply(data_train, is.numeric)], scale))
data_scaled_test <- as.data.frame(sapply(data_test[, sapply(data_test, is.numeric)], scale))
```

```{r NNModelCreation, cache = TRUE}
set.seed(7)

# Define tune grid. Because of time and computing limitations we only use a small grid
tune_grid <- expand.grid(
  size = seq(from = 2, to = 10, by = 2),
  decay = seq(from = 0.1, to = 1, by = 0.1)
)

# To speed up the training process we use parallel processing with the help of the doParallel package
cl <- makeCluster(detectCores())
registerDoParallel(cl)

# Define model
model_nn_1 <- train(wage ~ .,
  data = data_scaled_train,
  method = "nnet",
  maxit = 100,
  tuneGrid = tune_grid,
  trace = F,
  linout = 1
)

# Stop the cluster
stopCluster(cl)
```

### Model evaluation

```{r NNModelEvaluation}
# Use the created model to predict the wage on the test data
predictions_nn <- predict(model_nn_1, data_scaled_test)

# Evaluate the performance of the model
mse_nn <- mean((predictions_nn - data_scaled_test$wage)^2)
rmse_nn <- sqrt(mse_nn)
r_squared_nn <- cor(predictions_nn, data_scaled_test$wage)^2

# Print results
cat("RMSE of the tuned NN model:", rmse_nn, "\n")
cat("R-squared of the tuned NN model:", r_squared_nn, "\n")
```

When we look at the R-squared value of the neural network model we can see that the model is able to explain about 14% of the variance in the data. This is very bad. This could be because the model is not able to capture the complexity of the data. We could try to use a more complex model like a random forest or a gradient boosting model.
When we look at the RMSE value of 0.93 we can see that the model is not able to predict the wage very good. This means that the model is not accurate and makes a lot of mistakes when predicting the wage. This is also a sign that the model is not able to capture the complexity of the data.

```{r}
print(model_nn_1)
caret::RMSE(data_scaled_test$wage, predictions_nn)
```
When we use print to look at the model we can see, that it used the size 2 and the decay of 1. Because of that we can adjust our grid to only use these values. This will speed up our work. In addition we can now narrow our grid to values near the current ones to maybe find even better values.

## NN Model with all variables
In the first NN model we only used the numeric values. Now we will use all variables to see if the model can predict the wage better. We will also scale the numeric variables and combine them with the categorical variables.

```{r}
# Scale test and train data
data_scaled_train <- as.data.frame(sapply(data_train[, sapply(data_train, is.numeric)], scale))
data_scaled_test <- as.data.frame(sapply(data_test[, sapply(data_test, is.numeric)], scale))

# Scale test and train data and add the categorical variables
# Identify numeric columns only
numeric_columns <- sapply(data_train, is.numeric)

# Scale the numeric data
data_scaled_train <- scale(data_train[, numeric_columns])

# Convert the scaled data back to a data frame
data_scaled_train <- as.data.frame(data_scaled_train)

# Combine the scaled data with the character variable
data_scaled_train <- cbind(data_train[, !numeric_columns], data_scaled_train)

# Identify numeric columns only
numeric_columns <- sapply(data_test, is.numeric)

# Scale the numeric data
data_scaled_test <- scale(data_test[, numeric_columns])

# Convert the scaled data back to a data frame
data_scaled_test <- as.data.frame(data_scaled_test)

# Combine the scaled data with the character variable
data_scaled_test <- cbind(data_test[, !numeric_columns], data_scaled_test)
```

```{r NNModelCreation2, cache = TRUE}
set.seed(7)

# Define tune grid. Because of time and computing limitations we only use a small grid
tune_grid <- expand.grid(
  size = seq(from = 2, to = 10, by = 2),
  decay = seq(from = 0.1, to = 1, by = 0.1)
)

# To speed up the training process we use parallel processing with the help of the doParallel package
cl <- makeCluster(detectCores())
registerDoParallel(cl)

# Define model
model_nn_2 <- train(wage ~ .,
  data = data_scaled_train,
  method = "nnet",
  maxit = 100,
  tuneGrid = tune_grid,
  trace = F,
  linout = 1
)

# Stop the cluster
stopCluster(cl)
```

### Model evaluation

```{r NNModelEvaluation2}
# Use the created model to predict the wage on the test data
predictions_nn <- predict(model_nn_2, data_scaled_test)

# Evaluate the performance of the model
mse_nn <- mean((predictions_nn - data_scaled_test$wage)^2)
rmse_nn <- sqrt(mse_nn)
r_squared_nn <- cor(predictions_nn, data_scaled_test$wage)^2

# Print results
cat("RMSE of the tuned NN model:", rmse_nn, "\n")
cat("R-squared of the tuned NN model:", r_squared_nn, "\n")
```
As we can see above the model has improved significantly.
When we look at the R-squared value of the neural network model we can see that the model is able to explain about 52% of the variance in the data. This is a big improvement compared to the first model. This is a sign that the model is able to capture the complexity of the data better. This is also a sign that the model is able to generalize better to new data.
When we look at the RMSE value of 0.69 we can see that the model is able to predict the wage better. This means that the model is more accurate and makes less mistakes when predicting the wage. This is also a sign that the model is able to capture the complexity of the data better.
But still the model isn't very good at predicting the wage.

```{r}
print(model_nn_2)
```

We can see that because of the additional variables the size is now 4 compared to 2 in the first model. 
We saw, that the additional data significantly improved the model. Because of that we should try to adjust our grid to values near the current ones to maybe find even better values, especially for the decay because it is at the maximum value we provided.

```{r}
# Compare values from test to train data

# Extract the resampling results
resampling_results <- model_nn_2$resample

# Extract the largest RMSE value
max_rmse_value <- max(resampling_results$RMSE)

# Print the largest RMSE value
cat("Largest RMSE value from NN training model:", max_rmse_value, "\n")
cat("RMSE value of the test data:", caret::RMSE(data_scaled_test$wage, predictions_nn), "\n")
```
From the values above we can see, that the RMSE value from the test data is lower than the RMSE value for the training data. This is expected because it is unseen data. Because the difference is not huge we can say, that our model is not overfitted. This is a good sign because it means that the model is able to generalize to new data.

## NN Model improvement
In this part we want to improve out NN model with all variables because it performed the best. We will try to narrow down the grid to find even better values for the model.
From our previous model we know that the size is 4 and the decay is 1. With this information we adjust our grid.


```{r NNModelCreation3, cache = TRUE}
set.seed(7)

# Define tune grid. Because of time and computing limitations we only use a small grid
tune_grid <- expand.grid(
  size = seq(from = 3, to = 4, by = 1),
  decay = seq(from = 0.6, to = 1, by = 0.01)
)

# To speed up the training process we use parallel processing with the help of the doParallel package
cl <- makeCluster(detectCores())
registerDoParallel(cl)

# Define model
model_nn_3 <- train(wage ~ .,
  data = data_scaled_train,
  method = "nnet",
  maxit = 100,
  tuneGrid = tune_grid,
  trace = F,
  linout = 1
)

# Stop the cluster
stopCluster(cl)
```

### Model evaluation

```{r NNModelEvaluation3}
# Use the created model to predict the wage on the test data
predictions_nn <- predict(model_nn_3, data_scaled_test)

# Evaluate the performance of the model
mse_nn <- mean((predictions_nn - data_scaled_test$wage)^2)
rmse_nn <- sqrt(mse_nn)
r_squared_nn <- cor(predictions_nn, data_scaled_test$wage)^2

# Print results
cat("RMSE of the tuned NN model:", rmse_nn, "\n")
cat("R-squared of the tuned NN model:", r_squared_nn, "\n")
```

```{r}
print(model_nn_3)
```

As we can see the model has improved in the RSME from 0.80 to 0.69. This means that our model makes less error when predicting the wage. But in the same time our model got worse at the R-squared from 0.69 to 0.53. This means that our model is able to explain less variance in the data. This is a sign that our model is not able to capture the complexity of the data as good as before. This is a sign that our model is not able to generalize to new data as good as before.

## Model creation with XGBoost
Because XGBoost only works with numeric data we have to encode the remaining categorical variables with one hot encoding. We will also scale the data.
```{r}
# First we need to split the data into a training and a test set
set.seed(7)

# Define one-hot encoding function
dummy <- dummyVars(" ~ .", data = data)

# Perform one-hot encoding on the data
data_encoded <- data.frame(predict(dummy, newdata = data))

# Split the data into 70% training and 30% test data
trainIndex <- createDataPartition(data_encoded$wage, p = .7, list = FALSE)
data_train <- data_encoded[trainIndex, ]
data_test <- data_encoded[-trainIndex, ]
```

Now we need to scale de data. 
```{r}
# Scale test and train data
data_scaled_train <- as.data.frame(sapply(data_train[, sapply(data_train, is.numeric)], scale))
data_scaled_test <- as.data.frame(sapply(data_test[, sapply(data_test, is.numeric)], scale))
```

To find the best parameters for the XGBoost model we will use cross validation. We will use the `xgb.cv` function to find the best number of rounds. We will use the `xgb.train` function to train the model with the best number of rounds.
```{r XGBoostModelCreation}
params <- list(
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  objective = "reg:squarederror"
)

# Set seed
set.seed(7)

# Convert the data to xgb.DMatrix format
xgbtrain <- xgb.DMatrix(data = as.matrix(data_scaled_train[, -261]), label = data_scaled_train$wage)

# Perform cross-validation to find the optimal number of rounds
cv_model <- xgb.cv(
  params = params,
  data = xgbtrain,
  nrounds = 150,
  nfold = 5,
  early_stopping_rounds = 10,
  verbose = 0
)

# Train the XGBoost model with the optimal number of rounds
model_xgb <- xgb.train(
  params = params,
  data = xgbtrain,
  nrounds = cv_model$best_iteration
)
```

### Model evaluation
```{r XGBoostEvaluation}
# Check how good the model works on unseen data
predictions_xgb <- predict(model_xgb, as.matrix(data_scaled_test[, -261]))

# Output RMSE value
RMSE_xgb <- caret::RMSE(data_scaled_test$wage, predictions_xgb)
RMSE_xgb
```
When we look at the RMSE value of the model of 0.7 we can see that the model is a bit worse than the neural network model. Both models aren't really good but the XBoost is compared to the neural network a lot faster to train. This is a big advantage because we can try more models in the same time.

# Feature importance
In this part we look at the feature importance of the different models

## Neural Network

```{r}
# Compute permutation-based feature importance
importance1 <- varImp(model_nn_1, scale = FALSE)
importance2 <- varImp(model_nn_2, scale = FALSE)
importance3 <- varImp(model_nn_3, scale = FALSE)

# Print the feature importance as a table
print("Feature Importance Neural Network Model 1")
print(importance1)

print("Feature Importance Neural Network Model 2")
print(importance2)

print("Feature Importance Neural Network Model 3")
print(importance3)
```
In the neural networks we can see that if we include all variables in the model that the most important variable is that you live in the USA. We can also see that the feature student is very important, because it is on the second and third place. 
This is probably because students earn nothing or just a little bit of money. This is expected to have a high impact on the wage. What is also expected is that the different countries are important because the wage can vary a lot between different countries that the job role isn't most defining.

## XGBoost

```{r XGBoostFeatureImportance}
# Compute feature importance
importance_matrix <- xgb.importance(feature_names = colnames(data_scaled_train[, -261]), model = model_xgb)

# Print the feature importance as a table
datatable(importance_matrix, options = list(pageLength = 20), caption = "Feature Importance XGBoost Model")
```

As we can see from the feature importance table above the most important feature by far is if a person lives in the USA or not. 
With a gain of 26.5%. This is very surprising for me because I would have expected that the years of education or the age would be the most important features. 
But this is a sign that the model is able to capture the complexity of the data.  
As expected it is also important about 8% if the person is a student or not. This is probably because students earn nothing or just a little bit of money.
What also can be observed that in the top 10 features there are only 2 countries compared to 4 in the neural network model. 

## Conclusion
When comparing the feature importance of all four models we can see, that the feature USA is still the most important one next to the student feature. 
What also can be observed is that the top features in the neural networks have around 1.x percent in importance when the first value in the XGBoost model has over 26%. 
After this value the importance goes down a lot. What also can be sad is that the different model have sometimes the same features in the top 20 but not all.

# Own loan prediction
In this part we evaluate out future loan with our best performing model. This is the neural network in part 1.7.

```{r LoanPrediction}
# Define data frame for each group member to predict their loan

# Roman
data_roman <- data.frame(
  gender = "Male",
  age = "22-24",
  country = "Switzerland",
  education = "Bachelor’s degree",
  undergraduate_major = "Other",
  job_role = "Other",
  industry = "Shipping/Transportation",
  years_experience = "5-11",
  ML_atwork = "We have well established ML methods (i.e., models in production for more than 2 years)",
  Activities_Analyze.and.understand.data.to.influence.product.or.business.decisions = 1,
  Activities_Build.and.or.run.a.machine.learning.service.that.operationally.improves.my.product.or.workflows = 0,
  Activities_Build.and.or.run.the.data.infrastructure.that.my.business.uses.for.storing..analyzing..and.operationalizing.data = 0,
  Activities_Build.prototypes.to.explore.applying.machine.learning.to.new.areas = 1,
  Activities_Do.research.that.advances.the.state.of.the.art.of.machine.learning = 0,
  Activities_None.of.these.activities.are.an.important.part.of.my.role.at.work = 0,
  Notebooks_Kaggle.Kernels = 0,
  Notebooks_Google.Colab = 1,
  Notebooks_Azure.Notebook = 0,
  Notebooks_Google.Cloud.Datalab = 0,
  Notebooks_JupyterHub.Binder = 0,
  Notebooks_None = 0,
  cloud_Google.Cloud.Platform..GCP. = 0,
  cloud_Amazon.Web.Services..AWS. = 0,
  cloud_Microsoft.Azure = 1,
  cloud_IBM.Cloud = 0,
  cloud_Alibaba.Cloud = 0,
  cloud_I.have.not.used.any.cloud.providers = 0,
  Programming_Python = 0,
  Programming_R = 1,
  Programming_SQL = 1,
  Programming_Bash = 0,
  Programming_Java = 0,
  Programming_Javascript.Typescript = 0,
  Programming_Visual.Basic.VBA = 1,
  Programming_C.C.. = 0,
  Programming_MATLAB = 0,
  Programming_Scala = 0,
  Programming_Julia = 0,
  Programming_SAS.STATA = 0,
  Programming_language_used_most_often = "Other",
  ML_framework_Scikit.Learn = 0,
  ML_framework_TensorFlow = 1,
  ML_framework_Keras = 1,
  ML_framework_PyTorch = 1,
  ML_framework_Spark.MLlib = 0,
  ML_framework_H20 = 1,
  ML_framework_Caret = 1,
  ML_framework_Xgboost = 1,
  ML_framework_randomForest = 1,
  ML_framework_None = 0,
  Visualization_ggplot2 = 1,
  Visualization_Matplotlib = 0,
  Visualization_Altair = 0,
  Visualization_Shiny = 1,
  Visualization_Plotly = 1,
  Visualization_None = 0,
  percent_actively.coding = "25% to 49% of my time",
  How.long.have.you.been.writing.code.to.analyze.data. = "3-5 years",
  For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school.. = "< 1 year",
  Do.you.consider.yourself.to.be.a.data.scientist. = "Definitely not",
  data_Categorical.Data = 0,
  data_Genetic.Data = 0,
  data_Geospatial.Data = 0,
  data_Image.Data = 0,
  data_Numerical.Data = 0,
  data_Sensor.Data = 0,
  data_Tabular.Data = 1,
  data_text.Data = 0,
  data_Time.Series.Data = 0,
  data_Video.Data = 0,
  explainability.model_Examine.individual.model.coefficients = 1,
  explainability.model_examine.feature.correlations = 0,
  explainability.model_Examine.feature.importances = 1,
  explainability.model_Create.partial.dependence.plots = 0,
  explainability.model_LIME.functions = 0,
  explainability.model_SHAP.functions = 0,
  explainability.model_None.I.do.not.use.these.model.explanation.techniques = 0
)

# Akash

# Cyril

# Keijo
data_keijo <- data.frame(
  gender = "Male",
  age = "22-24",
  country = "Switzerland",
  education = "Bachelor’s degree",
  undergraduate_major = "Other",
  job_role = "Other",
  industry = "Accounting/Finance",
  years_experience = "5-11",
  ML_atwork = "We have well established ML methods (i.e., models in production for more than 2 years)",
  Activities_Analyze.and.understand.data.to.influence.product.or.business.decisions = 1,
  Activities_Build.and.or.run.a.machine.learning.service.that.operationally.improves.my.product.or.workflows = 0,
  Activities_Build.and.or.run.the.data.infrastructure.that.my.business.uses.for.storing..analyzing..and.operationalizing.data = 0,
  Activities_Build.prototypes.to.explore.applying.machine.learning.to.new.areas = 1,
  Activities_Do.research.that.advances.the.state.of.the.art.of.machine.learning = 0,
  Activities_None.of.these.activities.are.an.important.part.of.my.role.at.work = 0,
  Notebooks_Kaggle.Kernels = 0,
  Notebooks_Google.Colab = 1,
  Notebooks_Azure.Notebook = 0,
  Notebooks_Google.Cloud.Datalab = 0,
  Notebooks_JupyterHub.Binder = 0,
  Notebooks_None = 1,
  cloud_Google.Cloud.Platform..GCP. = 0,
  cloud_Amazon.Web.Services..AWS. = 1,
  cloud_Microsoft.Azure = 0,
  cloud_IBM.Cloud = 0,
  cloud_Alibaba.Cloud = 0,
  cloud_I.have.not.used.any.cloud.providers = 0,
  Programming_Python = 1,
  Programming_R = 1,
  Programming_SQL = 1,
  Programming_Bash = 1,
  Programming_Java = 1,
  Programming_Javascript.Typescript = 1,
  Programming_Visual.Basic.VBA = 0,
  Programming_C.C.. = 0,
  Programming_MATLAB = 0,
  Programming_Scala = 0,
  Programming_Julia = 0,
  Programming_SAS.STATA = 0,
  Programming_language_used_most_often = "Java",
  ML_framework_Scikit.Learn = 0,
  ML_framework_TensorFlow = 1,
  ML_framework_Keras = 0,
  ML_framework_PyTorch = 1,
  ML_framework_Spark.MLlib = 0,
  ML_framework_H20 = 1,
  ML_framework_Caret = 1,
  ML_framework_Xgboost = 1,
  ML_framework_randomForest = 1,
  ML_framework_None = 0,
  Visualization_ggplot2 = 1,
  Visualization_Matplotlib = 0,
  Visualization_Altair = 0,
  Visualization_Shiny = 1,
  Visualization_Plotly = 1,
  Visualization_None = 0,
  percent_actively.coding = "25% to 49% of my time",
  How.long.have.you.been.writing.code.to.analyze.data. = "1-2 years",
  For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school.. = "< 1 year",
  Do.you.consider.yourself.to.be.a.data.scientist. = "Definitely not",
  data_Categorical.Data = 0,
  data_Genetic.Data = 0,
  data_Geospatial.Data = 0,
  data_Image.Data = 0,
  data_Numerical.Data = 0,
  data_Sensor.Data = 0,
  data_Tabular.Data = 1,
  data_text.Data = 0,
  data_Time.Series.Data = 0,
  data_Video.Data = 0,
  explainability.model_Examine.individual.model.coefficients = 0,
  explainability.model_examine.feature.correlations = 0,
  explainability.model_Examine.feature.importances = 1,
  explainability.model_Create.partial.dependence.plots = 0,
  explainability.model_LIME.functions = 0,
  explainability.model_SHAP.functions = 0,
  explainability.model_None.I.do.not.use.these.model.explanation.techniques = 0
)

data_cyril <- data.frame(
  gender = "Male",
  age = "25-29",
  country = "Switzerland",
  education = "Bachelor’s degree",
  undergraduate_major = "Other",
  job_role = "Business Analyst",
  industry = "Insurance/Risk Assessment",
  years_experience = "0-1",
  ML_atwork = "No (we do not use ML methods)",
  Activities_Analyze.and.understand.data.to.influence.product.or.business.decisions = 1,
  Activities_Build.and.or.run.a.machine.learning.service.that.operationally.improves.my.product.or.workflows = 0,
  Activities_Build.and.or.run.the.data.infrastructure.that.my.business.uses.for.storing..analyzing..and.operationalizing.data = 0,
  Activities_Build.prototypes.to.explore.applying.machine.learning.to.new.areas = 0,
  Activities_Do.research.that.advances.the.state.of.the.art.of.machine.learning = 0,
  Activities_None.of.these.activities.are.an.important.part.of.my.role.at.work = 0,
  Notebooks_Kaggle.Kernels = 0,
  Notebooks_Google.Colab = 0,
  Notebooks_Azure.Notebook = 0,
  Notebooks_Google.Cloud.Datalab = 0,
  Notebooks_JupyterHub.Binder = 0,
  Notebooks_None = 1,
  cloud_Google.Cloud.Platform..GCP. = 0,
  cloud_Amazon.Web.Services..AWS. = 0,
  cloud_Microsoft.Azure = 0,
  cloud_IBM.Cloud = 0,
  cloud_Alibaba.Cloud = 0,
  cloud_I.have.not.used.any.cloud.providers = 1,
  Programming_Python = 0,
  Programming_R = 0,
  Programming_SQL = 0,
  Programming_Bash = 0,
  Programming_Java = 0,
  Programming_Javascript.Typescript = 0,
  Programming_Visual.Basic.VBA = 0,
  Programming_C.C.. = 0,
  Programming_MATLAB = 0,
  Programming_Scala = 0,
  Programming_Julia = 0,
  Programming_SAS.STATA = 0,
  Programming_language_used_most_often = "Other",
  ML_framework_Scikit.Learn = 0,
  ML_framework_TensorFlow = 0,
  ML_framework_Keras = 0,
  ML_framework_PyTorch = 0,
  ML_framework_Spark.MLlib = 0,
  ML_framework_H20 = 0,
  ML_framework_Caret = 0,
  ML_framework_Xgboost = 0,
  ML_framework_randomForest = 0,
  ML_framework_None = 1,
  Visualization_ggplot2 = 0,
  Visualization_Matplotlib = 0,
  Visualization_Altair = 0,
  Visualization_Shiny = 0,
  Visualization_Plotly = 0,
  Visualization_None = 1,
  percent_actively.coding = "0% of my time",
  How.long.have.you.been.writing.code.to.analyze.data. = "1-2 years",
  For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school.. = "< 1 year",
  Do.you.consider.yourself.to.be.a.data.scientist. = "Definitely not",
  data_Categorical.Data = 0,
  data_Genetic.Data = 0,
  data_Geospatial.Data = 0,
  data_Image.Data = 0,
  data_Numerical.Data = 0,
  data_Sensor.Data = 0,
  data_Tabular.Data = 0,
  data_text.Data = 0,
  data_Time.Series.Data = 0,
  data_Video.Data = 0,
  explainability.model_Examine.individual.model.coefficients = 0,
  explainability.model_examine.feature.correlations = 0,
  explainability.model_Examine.feature.importances = 0,
  explainability.model_Create.partial.dependence.plots = 0,
  explainability.model_LIME.functions = 0,
  explainability.model_SHAP.functions = 0,
  explainability.model_None.I.do.not.use.these.model.explanation.techniques = 1
)


# Calculate the mean and standard deviation of the wage column in the original training data. This is used for the descaling later on
mean_wage <- mean(data_train$wage, na.rm = TRUE)
sd_wage <- sd(data_train$wage, na.rm = TRUE)

prediction_roman <- predict(model_nn_3, newdata = data_roman)
# prediction_akash <- predict(model_nn_3, newdata = data_akash)
prediction_cyril <- predict(model_nn_3, newdata = data_cyril)
prediction_keijo <- predict(model_nn_3, newdata = data_keijo)

# Descale the prediction
prediction_roman_descaled <- prediction_roman * sd_wage + mean_wage
# prediction_akash_descaled <- prediction_akash * sd_wage + mean_wage
prediction_cyril_descaled <- prediction_cyril * sd_wage + mean_wage
prediction_keijo_descaled <- prediction_keijo * sd_wage + mean_wage


cat("Predicted loan Roman: ", prediction_roman_descaled)
# cat("Predicted loan Akash: ", prediction_akash)
cat("Predicted loan Cyril: ", prediction_cyril_descaled)
cat("Predicted loan Keijo: ", prediction_keijo_descaled)
```

