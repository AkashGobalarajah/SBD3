---
title: "Group 4 - Salary Prediction"
author: "Akash Gobalarajah, Cyril Alain Scheurmann, Keijo Alexander Nierula, Roman Krass"
date: "2024.03.14"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE, echo=FALSE}
# Getting started by changing the default output of echo to TRUE for the current document
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

# Create a list of packages to install and load into the work space
libraries <- c("readr", "tidyverse", "dlookr", "ggplot2", "xgboost", "dplyr", "caret", "doParallel")

# Install packages from the predefined libraries list
lapply(libraries, function(x) {
  if (!(x %in% installed.packages())) {
    install.packages(x)
  }
})

# Load libraries
lapply(libraries, library, quietly = TRUE, character.only = TRUE)

# Remove current environment
rm(list = ls())

# scientific notation: off
options(scipen = 999)

# Set working directory
setwd("C:/Users/roman/documents/repos/SBD3/Homework1")
```

# Project Setup {.unnumbered}

```{r importData}
# Set seed
set.seed(7)

# Importing data
load("data_wage.RData")
salaryData <- data
```

# Checking data for NA values {.unnumbered}
In this part we check our data for missing values. This is important because missing values can cause problems in the analysis and can lead to wrong results. 

<details>
<summary>Click to expand NA results</summary>


```{r naValues, out.width = "100%"}
# Check for missing values in each column of the data set and store the result in the "has_NAs" data frame
has_NAs <- as.data.frame(apply(data, 2, function(x) any(is.na(x))))

# Map matching column names for a more readable output
colnames(has_NAs) <- c("has_NA")

# Print the has_NAs data frame to check if there are any NA values in our data set
has_NAs
```
</details>
As we can see from the output above none of the columns contain any missing values. This is very good so we don't have to deal with the problem of missing values.


# Data description

``` {r}
dim(data)
```
The `dim` function shows us, that we have 10'809 and 78 variables in our data set. We have so many variables because the data we have has already one hot encoded some variables.

<details>
<summary>Click to expand to see data structure</summary>

```{r scructure}
# Check the structure of the data
str(data)
```
</details>
  
<details>
<summary>Click to expand to see the summary of all variables</summary>

```{r summary()}
# Check the summary of the data
summary(data)
```
</details>
When we look at the summary the variable `wage` has a minimum value of 0 and a maximum value of 551'774. This is a very big range and we have to check if there are any outliers in the data set. The big range can also be caused, because the dataset includes data from students to experts which indeed can have a big range in their salary.
All other variables look good. This is because a lot of variables are categorical variables which were one hot encoded. 

## How many numeric and how many categorical variables are included in the data? What categorical variable has the most levels in it?

```{r}
num_vars <- sum(sapply(data, is.numeric))
cat_vars <- sum(sapply(data, is.factor))

# Print counts
cat("Number of numeric variables:", num_vars, "\n")
cat("Number of categorical variables:", cat_vars, "\n")
```

Reporting the structure of the data set shows us, that we have `r as.character(cat_vars)` character columns and `r as.character(num_vars)` number columns.
Because we have `r as.character(cat_vars)` we cannot use all models because they only work with numeric data. If we could like to use a model like XGBoost we have to encode these categorical variables with one hot encoding so we end with only numeric variables.

Eventuell noch `diagnose` benutzen

## Check dependent variable wage

```{r}
# Plot histogram of wage
ggplot(data, aes(x = wage)) +
  geom_histogram(bins = 60, fill = "lightblue", color = "black") +
  scale_x_continuous(breaks = seq(min(data$wage), max(data$wage), by = 50000)) +
  labs(title = "Histogram of wage", x = "Wage", y = "Frequency") +
  theme_minimal(base_size = 17)
```

Wehen we look at the histogram of the wage we can see that that most of the values are between 0 and 150'000. There are also some values between 150'000 and 250'000. The values above 250'000 are very rare. This is also a bit as expected because wages above 200'000 are not so common as wages to 150'000.

## Visualization to display the distribution of the categorial features

<details>
<summary>Click to expand variable distribution</summary>

```{r distribution-categorial}
# Function to plot bar plots for each categorical variable
plot_categorical_associations <- function(data) {
  categorical_vars <- sapply(data, is.factor) # or is.factor, depending on how data is loaded
  data_categorical <- data[, categorical_vars]

  for (var in names(data_categorical)) {
    # Create the plot and assign it to a variable
    plot <- ggplot(data, aes(x = !!sym(var))) +
      geom_bar(position = "stack") +
      labs(title = paste("Distribution of", var), x = var, y = "Count") +
      theme_minimal(base_size = 17) +
      scale_fill_manual(values = c("#105F77", "#f4a460")) +
      theme(
        axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 18)
      )

    # Print the plot
    print(plot)
  }
}

# Apply the function to the loan dataset
plot_categorical_associations(data)
```

</details>

# Model creation {.unnumbered}

## Splitting the data into training and test set
The first step before we can start with the model creation is to split the data into a training and a test set. This is important because we need to test our model on data it has never seen before. This is important to see if the model is able to generalize to new data. It also helpts to identify overfitting if the model is overfitted to the training data.
```{r}
# First we need to split the data into a training and a test set
set.seed(7)

# Split the data into 70% training and 30% test data
trainIndex <- createDataPartition(data$wage, p = .7, list = FALSE)
data_train <- data[trainIndex, ]
data_test <- data[-trainIndex, ]
```

## Model creation with neural network

```{r}
# Scale test and train data
data_scaled_train <- as.data.frame(sapply(data_train[, sapply(data_train, is.numeric)], scale))
data_scaled_test <- as.data.frame(sapply(data_test[, sapply(data_test, is.numeric)], scale))
```

```{r NNModelCreation}
set.seed(7)

# Define tune grid. Because of time and computing limitations we only use a small grid
tune_grid <- expand.grid(
  size = seq(from = 2, to = 10, by = 2),
  decay = seq(from = 0.1, to = 1, by = 0.1)
)

# To speed up the training process we use parallel processing with the help of the doParallel package
cl <- makeCluster(detectCores())
registerDoParallel(cl)

# Define model
model_nn <- train(wage ~ .,
  data = data_scaled_train,
  method = "nnet",
  maxit = 100,
  tuneGrid = tune_grid,
  trace = F,
  linout = 1
)

# Stop the cluster
stopCluster(cl)
```

## Model evaluation

```{r NNModelEvaluation}
# Use the created model to predict the wage on the test data
predictions_nn <- predict(model_nn, data_scaled_test)

# Evaluate the performance of the model
mse_nn <- mean((predictions_nn - data_scaled_test$wage)^2)
rmse_nn <- sqrt(mse_nn)
r_squared_nn <- cor(predictions_nn, data_scaled_test$wage)^2

# Print results
cat("RMSE of the tuned NN model:", rmse_nn, "\n")
cat("R-squared of the tuned NN model:", r_squared_nn, "\n")
```

When we look at the R-squared value of the neural network model we can see that the model is able to explain 14% of the variance in the data. This is very bad. This could be because the model is not able to capture the complexity of the data. We could try to use a more complex model like a random forest or a gradient boosting model.