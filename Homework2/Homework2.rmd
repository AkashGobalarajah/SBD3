---
title: "Group 4 - Disneyland Text Mining"
author: "Akash Gobalarajah, Cyril Alain Scheurmann, Keijo Alexander Nierula, Roman Krass"
date: "2024.03.14"
output:
html_document:
toc: true
toc_depth: 2
toc_float: true
number_sections: true
---

```{r setup, include=FALSE}
# Getting started by changing the default output of echo to TRUE for the current document
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

# Create a list of packages to install and load into the work space
libraries = c("tm", "SnowballC", "wordcloud", "RColorBrewer", "syuzhet", "cld2", "topicmodels", "quanteda", "tidyverse", "tidytext", "reshape2")

# Install packages from the predefined libraries list
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
  install.packages(x)
})


# Load libraries
lapply(libraries, library, quietly = TRUE, character.only = TRUE)

# Remove current environment
rm(list=ls())

#scientific notation: off
options(scipen=999)

# TEST 
```

```{r, echo=FALSE}
# Load the data
load("/Users/keijo/Documents/FH/Sem6/SBD3/git/SBD3/Homework2/Disneyland.rda")

# Check the structure of the data
str(reviews)

head(reviews)

# How many review_locations:
table(reviews$Reviewer_Location)
```


# 1. What can you tell us about the customers that write reviews? 

* We know that the customers are from all over the world. The most reviews are from the United States, followed by the United Kingdom and Australia. 


# 2. What do the visitors talk about in their reviews and how does it relate to sentiment/ratings? 


## Pre-prosessing the data
```{r}
# Are there other languages than english?
languages <- detect_language(reviews$Review_Text)
table(languages)

nrow(reviews)
# Filter the reviews to only include english reviews
reviews <- reviews[languages == "en", ]

# Check if there are duplicate reviews
duplicate_rows <- duplicated(reviews)

# It seems we have 12 duplicates
print(sum(duplicate_rows))

# Removing the duplicates
reviews <- unique(reviews)


# Check if there are any missing values
missing_values <- colSums(is.na(reviews))
print(missing_values)

# Remove the reviews missing values (year and year_month are missing 2613 times)
reviews <- reviews[complete.cases(reviews), ]
```

To get an overview of what the most used words in the reviews are, we are going to make a wordcloud:

```{r}
TODO


# # Tokenize the reviews
# tokens <- tokens(reviews$Review_Text,
#                  remove_punct = TRUE,
#                  remove_symbols = TRUE,
#                  remove_numbers = TRUE,
#                  remove_url = TRUE,
#                  remove_separators = TRUE)
# extended_stopwords <- c(stopwords("english"), "disney", "disneyland", "park", "time", "day", "get", "go")
# 
# # Remove common stopwords
# tokens <- tokens_select(tokens, pattern = extended_stopwords, selection = "remove")
# 
# # transform to lowercase
# tokens <- tokens_tolower(tokens)
# 
# # Stem all words
# tokens <-tokens_wordstem(tokens)
# 
# # Create n-grams of any length
# tokens <- tokens_ngrams(tokens, n = 1:2)
# 
# # Create Document-feature-matrix
# matrix <-dfm(tokens)
# 
# # Create LDA model
# reviews_lda <- LDA(matrix, k = 3, control = list(seed = 1111))
# 
# reviews_lda_td <- tidy(reviews_lda)
# reviews_lda_td
# 
# # Extract top-terms per topic
# top_terms <- reviews_lda_td %>%
#   group_by(topic) %>%
#   top_n(8, beta) %>%
#   ungroup() %>%
#   arrange(topic, -beta)
# 
# 
# top_terms %>%
#   mutate(term = reorder(term, beta)) %>%
#   ggplot(aes(term, beta, fill = factor(topic))) +
#   geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
#   facet_wrap(~ topic, scales = "free") +
#   coord_flip()
```

First we want to find out what the visitors talk about in the reviews that have a positive sentiment and a rating of 4 and 5.
```{r}
# Calculate sentiment for the reviews
reviews$sentiment.syuzhet <- get_sentiment(reviews$Review_Text,
                                                  method="syuzhet",
                                                  lang="english")

good_sentiment_reviews <- subset(reviews, sentiment.syuzhet > 0 & Rating >= 4)

tokens <- tokens(good_sentiment_reviews$Review_Text,
                 remove_punct = TRUE,
                 remove_symbols = TRUE,
                 remove_numbers = TRUE,
                 remove_url = TRUE,
                 remove_separators = TRUE)
extended_stopwords <- c(stopwords("english"), "disney", "disneyland", "park", "time", "day", "get", "go", "one")

# Remove common stopwords
tokens <- tokens_select(tokens, pattern = extended_stopwords, selection = "remove")

# transform to lowercase
tokens <- tokens_tolower(tokens)

# Stem all words
tokens <-tokens_wordstem(tokens)

# Create n-grams of any length
tokens <- tokens_ngrams(tokens, n = 1:2)

# Create Document-feature-matrix
matrix <-dfm(tokens)

# Create LDA model
reviews_lda <- LDA(matrix, k = 3, control = list(seed = 1111))

reviews_lda_td <- tidy(reviews_lda)
reviews_lda_td

# Extract top-terms per topic
top_terms <- reviews_lda_td %>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

Next we create a topic model for negative sentiment reviews with a rating of 1 and 2:
```{r}
bad_sentiment_reviews <- subset(reviews, sentiment.syuzhet < 0 & Rating <= 2)
tokens_bad <- tokens(bad_sentiment_reviews$Review_Text,
                 remove_punct = TRUE,
                 remove_symbols = TRUE,
                 remove_numbers = TRUE,
                 remove_url = TRUE,
                 remove_separators = TRUE)

tokens_bad <- tokens_select(tokens_bad, pattern = extended_stopwords, selection = "remove")
tokens_bad <- tokens_tolower(tokens_bad)
tokens_bad <-tokens_wordstem(tokens_bad)
tokens_bad <- tokens_ngrams(tokens_bad, n = 1:2)
matrix_bad <-dfm(tokens_bad)
reviews_lda_bad <- LDA(matrix_bad, k = 3, control = list(seed = 1111))
reviews_lda_bad_td <- tidy(reviews_lda_bad)

top_terms_bad <- reviews_lda_bad_td %>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms_bad %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

```{r}
# 
# # First we reate a word cloud to visualize the most common words in the reviews
# reviews_corpus <- Corpus(VectorSource(reviews$Review_Text))
# 
# # Preprocessing the text data:
# toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
# reviews_corpus <- tm_map(reviews_corpus, toSpace, "/")
# reviews_corpus <- tm_map(reviews_corpus, toSpace, "@")
# reviews_corpus <- tm_map(reviews_corpus, toSpace, "\\|")
# reviews_corpus <- tm_map(reviews_corpus, content_transformer(tolower))
# reviews_corpus <- tm_map(reviews_corpus, removeNumbers)
# reviews_corpus <- tm_map(reviews_corpus, removeWords, c("disney", "park", "disneyland", stopwords("english")))
# reviews_corpus <- tm_map(reviews_corpus, removePunctuation)
# reviews_corpus <- tm_map(reviews_corpus, stripWhitespace)
# 
# # # Creating a term document matrix
# term.matrix <- TermDocumentMatrix(reviews_corpus)
# 
# # Remove terms that occur in less than 1% of the documents - because the dataset was too large to process (R ran out of memory)
# term.matrix <- removeSparseTerms(term.matrix, 0.99)
# 
# # convert to a regular matrix
# term.matrix <- as.matrix(term.matrix)
# 
# v <- sort(rowSums(term.matrix),decreasing=TRUE)
# data <- data.frame(word = names(v),freq=v)
# head(data, 10)
# 
# set.seed(17)
# wordcloud(words = data$word, freq = data$freq, min.freq = 1,
#           max.words=200, random.order=FALSE, rot.per=0.35,
#           colors=brewer.pal(8, "Dark2"))


```

```{r}
# #Now we create a wordcloud for bad sentiment reviews
# 
# # Select only the reviews with a rating of 1 or 2
# reviews_corpus_negative <- reviews_corpus[reviews$Rating %in% c(1, 2)]
# 
# reviews_corpus_negative$sentiment.syuzhet <- get_sentiment(reviews_corpus_negative,
#                                                   method="syuzhet",
#                                                   lang="english")
# 
# # Create a word cloud for the review with a negative sentiment
# reviews_corpus_negative <- reviews_corpus[reviews_corpus$sentiment.syuzhet < 0]
# 
# # Creating a term document matrix
# term.matrix.negative <- TermDocumentMatrix(reviews_corpus_negative)
# term.matrix.negative <- as.matrix(term.matrix.negative)
# 
# v <- sort(rowSums(term.matrix.negative),decreasing=TRUE)
# data <- data.frame(word = names(v),freq=v)
# 
# set.seed(17)
# wordcloud(words = data$word, freq = data$freq, min.freq = 1,
#           max.words=200, random.order=FALSE, rot.per=0.35,
#           colors=brewer.pal(8, "Dark2"))


```

TODO: summarize the findings from the topic modelling 







# 3. What differences can you detect for the three different locations and are there any interesting trends over time? 

TODO:
* Group the data by Disneyland_Branch and analyze differences in review topics and sentiments across different locations.
* Plot the trends in ratings and sentiment scores over time for each location.
* Perform topic modeling separately for each location to detect unique trends or issues.
* Compare the results to identify commonalities and differences between the locations.

```{r}
# We group the reviews by branch to analyze differences in ratings and sentiments between the branches
reviews_by_location <- reviews %>%
  group_by(Branch) %>%
  summarise(
    avg_rating = mean(Rating),
    count = n(),
    avg_sentiment = mean(sentiment.syuzhet)
  )

# Plotting the average ratings and sentiment scores for the three branches
ggplot(reviews_by_location, aes(x=Branch, y=avg_rating)) +
  geom_bar(stat="identity") + 
  theme_minimal() + 
  ggtitle("Average Rating by Disneyland Branch")

# Plotting the average sentiment scores for the three branches
ggplot(reviews_by_location, aes(x=Branch, y=avg_sentiment)) +
  geom_bar(stat="identity") + 
  theme_minimal() + 
  ggtitle("Average Sentiment by Disneyland Branch")

# Plotting the average ratings over time for the three branches
reviews %>%
  group_by(Year_Month, Branch) %>%
  summarise(avg_rating = mean(Rating)) %>%
  ggplot(aes(x=Year_Month, y=avg_rating, color=Branch)) +
  geom_line() +
  theme_minimal() +
  ggtitle("Average Rating Over Time by Disneyland Branch")

# Plotting the average sentiment scores over time for the three branches
reviews %>%
  group_by(Year_Month, Branch) %>%
  summarise(avg_sentiment = mean(sentiment.syuzhet)) %>%
  ggplot(aes(x=Year_Month, y=avg_sentiment, color=Branch)) +
  geom_line() +
  theme_minimal() +
  ggtitle("Average Sentiment of Reviews Over Time by Disneyland Branch")

```


# 4. What specific advice can you give to our park management based on your analysis? How can we integrate the analysis of reviews in our internal processes, can you think of any data products that would be of value for us?

TODO:
* Summarize the insights gained from the previous analyses.
* Provide actionable recommendations, such as:
  * Areas needing improvement based on negative sentiment topics.
  * Successful aspects highlighted in positive reviews.
* Suggest integrating automated sentiment and topic analysis into the review monitoring system to quickly identify emerging issues or trends.
* Propose creating dashboards for real-time sentiment and topic tracking.

```{r}

```
